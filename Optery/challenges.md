# Preparaci√≥n para Full Stack Developer - Optery
Job Post: https://www.workatastartup.com/jobs/82853

## üìã Requerimientos T√©cnicos del Puesto

### Requerimientos Obligatorios
- **Node.js**: 2+ a√±os de experiencia en proyectos significativos
- **Programaci√≥n As√≠ncrona**: Entendimiento profundo de async/await, Promises, callbacks
- **Arquitectura Event-Driven**: Manejo de eventos y arquitecturas basadas en eventos
- **Web Scraping**: Experiencia con crawlers y herramientas de automatizaci√≥n web
- **Bases de Datos**:
  - SQL: PostgreSQL
  - NoSQL: Redis, Elasticsearch, MongoDB
- **REST APIs**: Desarrollo e integraci√≥n con servicios de terceros
- **Seguridad de Datos**: Best practices para manejo seguro de datos de usuarios
- **Frontend B√°sico**: HTML, CSS, conocimientos b√°sicos de desarrollo frontend
- **Trabajo Remoto**: Capacidad de trabajo independiente

### Requerimientos Bonus (Opcionales)
- **Puppeteer**: Automatizaci√≥n y scraping web
- **Generative AI**: Uso de IA para mejorar automatizaci√≥n de workflows
- **Python y Django**: Familiaridad con el stack
- **Message Brokers**: RabbitMQ, Kafka
- **Cloud & Containerizaci√≥n**: Kubernetes, AWS, GCP, Azure

### Stack Tecnol√≥gico de Optery
- **Backend**: Python, Django, Node.js
- **Frontend**: React
- **Infraestructura**: AWS, Kubernetes (EKS), Grafana, Loki

---

## üéØ Retos de Preparaci√≥n

### NIVEL 1: FUNDAMENTOS (F√°cil)

> **Leyenda**: üî¥ = CR√çTICO para la vacante | ‚ö™ = OPCIONAL/BONUS

#### üî¥ Reto 1.1: Async/Await B√°sico
Crea un programa que:
- Haga 3 peticiones HTTP a diferentes APIs p√∫blicas (ejemplo: JSONPlaceholder, OpenWeather, etc.)
- Use async/await correctamente
- Maneje errores con try/catch
- Muestre los resultados en consola

#### üî¥ Reto 1.2: Servidor REST API Simple
Desarrolla una API REST con Node.js (Express) que:
- Tenga endpoints CRUD para "productos" (GET, POST, PUT, DELETE)
- Use arrays en memoria (sin base de datos)
- Retorne respuestas JSON correctas
- Implemente validaci√≥n b√°sica de datos

#### üî¥ Reto 1.3: Conexi√≥n a PostgreSQL
Crea una aplicaci√≥n que:
- Se conecte a PostgreSQL usando `pg` o `node-postgres`
- Cree una tabla de "usuarios" (id, nombre, email, fecha_creacion)
- Implemente operaciones CRUD b√°sicas
- Use prepared statements para prevenir SQL injection

#### üî¥ Reto 1.4: Web Scraper B√°sico
Desarrolla un scraper que:
- Extraiga t√≠tulos y links de una p√°gina de noticias
- Use `axios` o `node-fetch` para obtener el HTML
- Use `cheerio` para parsear el HTML
- Guarde los resultados en un archivo JSON

---

### NIVEL 2: INTERMEDIO (Medio)

> **Leyenda**: üî¥ = CR√çTICO para la vacante | ‚ö™ = OPCIONAL/BONUS

#### üî¥ Reto 2.1: Event Emitter Custom
Crea un sistema de eventos que:
- Implemente un EventEmitter personalizado
- Simule un sistema de logs con diferentes niveles (info, warning, error)
- Tenga m√∫ltiples listeners que reaccionen a diferentes eventos
- Implemente un sistema de prioridad para los listeners

#### üî¥ Reto 2.2: API con Rate Limiting
Desarrolla una API que:
- Implemente rate limiting (m√°ximo 10 requests por minuto por IP)
- Use Redis para almacenar los contadores
- Retorne headers apropiados (X-RateLimit-Remaining, X-RateLimit-Reset)
- Maneje errores 429 (Too Many Requests) correctamente

#### üî¥ Reto 2.3: Web Crawler con Queue
Crea un crawler que:
- Inicie desde una URL seed
- Extraiga todos los links de la p√°gina
- Siga los links de manera recursiva (m√°ximo 3 niveles de profundidad)
- Use una cola (queue) para manejar las URLs a visitar
- Evite visitar la misma URL dos veces
- Respete robots.txt
- Implemente delays entre requests

#### üî¥ Reto 2.4: Scraper con Puppeteer
Desarrolla un scraper que:
- Use Puppeteer para cargar p√°ginas con JavaScript
- Navegue por un sitio con paginaci√≥n
- Extraiga datos de m√∫ltiples p√°ginas
- Tome screenshots de las p√°ginas visitadas
- Maneje timeouts y errores de red

#### ‚ö™ Reto 2.5: Data Pipeline con MongoDB
Crea un pipeline que:
- Lea datos de una API p√∫blica
- Transforme los datos (limpie, normalice)
- Almacene los datos en MongoDB
- Implemente √≠ndices apropiados
- Use aggregation pipeline para generar reportes

#### üî¥ Reto 2.6: Clasificaci√≥n de Formularios con IA
Desarrolla un sistema que:
- Use OpenAI API (o similar) para analizar p√°ginas HTML
- Identifique si una p√°gina contiene un formulario de opt-out/unsubscribe
- Clasifique el tipo de formulario (email, SMS, data removal, etc.)
- Extraiga los campos requeridos del formulario
- Genere un JSON estructurado con la informaci√≥n del formulario
- Maneje errores de la API y respuestas ambiguas
- Implemente cach√© de resultados para URLs ya analizadas

#### üî¥ Reto 2.7: Integraci√≥n Full Stack (Node.js + Django + React)
Crea un sistema h√≠brido que:
- **Backend Principal (Node.js + Express)**:
  - API REST para gestionar trabajos de scraping
  - Reciba solicitudes de usuarios
  - Env√≠e jobs a un servicio Django
- **Servicio de Procesamiento (Python + Django)**:
  - Cree una API Django simple con un endpoint
  - Procese datos pesados (ej: an√°lisis de texto, ML simple)
  - Devuelva resultados al servicio Node.js
- **Frontend (React)**:
  - Interfaz para crear solicitudes de scraping
  - Visualice el estado de los jobs (pendiente, procesando, completado)
  - Muestre resultados en una tabla o tarjetas
- **Comunicaci√≥n**:
  - Node.js se comunica con Django v√≠a HTTP
  - Usa PostgreSQL compartida entre ambos servicios
  - Implementa autenticaci√≥n b√°sica entre servicios

---

### NIVEL 3: AVANZADO (Dif√≠cil)

> **Leyenda**: üî¥ = CR√çTICO para la vacante | ‚ö™ = OPCIONAL/BONUS

#### üî¥ Reto 3.1: Sistema de Job Queue Distribuido
Desarrolla un sistema que:
- Use Bull o BullMQ (basado en Redis) para job queues
- Implemente workers que procesen jobs en paralelo
- Maneje reintentos autom√°ticos en caso de fallo
- Implemente prioridades de jobs
- Tenga un dashboard para monitorear el estado de los jobs
- Maneje dead letter queues para jobs fallidos

#### üî¥ Reto 3.2: Crawler Robusto con Anti-Ban
Crea un web crawler que:
- Use Puppeteer con stealth plugin para evitar detecci√≥n
- Implemente rotaci√≥n de User-Agents
- Use proxies rotativos
- Maneje CAPTCHAs (detecci√≥n y logging)
- Implemente backoff exponencial en caso de errores
- Use headless browser con fingerprinting aleatorio
- Guarde el progreso para poder reanudar despu√©s de interrupciones

#### üî¥ Reto 3.3: API de Opt-Out Automatizado
Desarrolla un sistema que:
- Permita a usuarios registrar sus datos personales
- Busque esos datos en sitios "data brokers" simulados
- Genere solicitudes de eliminaci√≥n automatizadas
- Verifique que la eliminaci√≥n fue exitosa
- Env√≠e notificaciones al usuario sobre el progreso
- Use PostgreSQL para almacenar usuarios y requests
- Use Redis para cach√© de resultados de b√∫squeda

#### ‚ö™ Reto 3.4: Sistema de Scraping Distribuido
Implementa un sistema que:
- Use RabbitMQ o Kafka para distribuir tareas de scraping
- Tenga m√∫ltiples workers que consuman de la cola
- Implemente un coordinator que distribuya el trabajo
- Maneje fallos de workers (health checks)
- Use Elasticsearch para almacenar y buscar los datos scrapeados
- Implemente deduplicaci√≥n de datos
- Tenga m√©tricas y logging centralizado

#### üî¥ Reto 3.5: Automation Workflow con IA
Crea un sistema que:
- Use un modelo de lenguaje (OpenAI API o similar) para entender p√°ginas web
- Identifique autom√°ticamente formularios de opt-out
- Genere estrategias de llenado de formularios
- Valide que los formularios fueron enviados correctamente
- Aprenda de errores previos
- Use Puppeteer para automatizaci√≥n del navegador
- Implemente logging detallado del proceso

#### ‚ö™ Reto 3.6: Infraestructura con Docker & Kubernetes
Desarrolla una aplicaci√≥n que:
- Contenga un API, workers, y base de datos
- Cree Dockerfiles optimizados para cada servicio
- Use Docker Compose para desarrollo local
- Cree manifiestos de Kubernetes (Deployments, Services, ConfigMaps)
- Implemente health checks y readiness probes
- Configure auto-scaling basado en carga
- Use secrets para credenciales sensibles

---

## üîê Desaf√≠os de Seguridad

### üî¥ Desaf√≠o S1: Protecci√≥n de Datos Personales
Implementa un sistema que:
- Encripte datos sensibles en PostgreSQL
- Use bcrypt para hashear passwords
- Implemente JWT para autenticaci√≥n
- Valide y sanitice todos los inputs
- Prevenga SQL injection, XSS, y CSRF

### üî¥ Desaf√≠o S2: Manejo Seguro de Credenciales
Crea una aplicaci√≥n que:
- Nunca almacene credenciales en c√≥digo
- Use variables de entorno
- Implemente vault para secretos (opcional: HashiCorp Vault)
- Rote credenciales peri√≥dicamente
- Audite accesos a datos sensibles

---

## üìä Proyecto Final Integrador

### üî¥ Sistema Completo de Data Privacy
Construye una versi√≥n simplificada de Optery que:

**Features del MVP:**
1. **User Management**
   - Registro y login de usuarios
   - Almacenamiento seguro de datos personales
   
2. **Data Discovery**
   - Crawler que busque el nombre del usuario en 5 sitios simulados
   - Detecte presencia de datos personales
   - Genere un reporte con screenshots

3. **Removal Automation**
   - Identifique formularios de opt-out
   - Rellene formularios autom√°ticamente con Puppeteer
   - Env√≠e solicitudes de eliminaci√≥n
   
4. **Verification & Reporting**
   - Verifique que los datos fueron removidos
   - Genere reportes visuales con before/after
   - Notifique al usuario del progreso

5. **API & Frontend**
   - REST API completa documentada
   - Frontend simple en React para visualizar reportes
   - Dashboard con estad√≠sticas

**Requisitos T√©cnicos:**
- Backend: Node.js + Express
- Base de Datos: PostgreSQL + Redis
- Automation: Puppeteer
- Queue: Bull/BullMQ
- Frontend: React (b√°sico)
- Containerizaci√≥n: Docker
- Testing: Jest + Supertest
- Documentaci√≥n: Swagger/OpenAPI

**Criterios de √âxito:**
- C√≥digo limpio y bien estructurado
- Manejo apropiado de errores
- Tests unitarios e integraci√≥n
- Documentaci√≥n clara
- Seguridad implementada
- Performance optimizada
- Logs y monitoring

---

## üìö Recursos de Estudio Recomendados

### Node.js & Async Programming
- Documentaci√≥n oficial de Node.js
- "Node.js Design Patterns" por Mario Casciaro
- Event Loop explicado

### Web Scraping
- Documentaci√≥n de Puppeteer
- Documentaci√≥n de Cheerio
- Best practices de web scraping √©tico

### Bases de Datos
- PostgreSQL Tutorial oficial
- Redis Documentation
- MongoDB University (cursos gratuitos)

### Arquitectura
- Event-Driven Architecture patterns
- Microservices patterns
- Queue-based systems

### Seguridad
- OWASP Top 10
- Secure coding practices
- Data privacy regulations (GDPR basics)

---

## ‚úÖ Checklist de Preparaci√≥n

- [ ] Dominar async/await y Promises en Node.js
- [ ] Crear al menos 3 web scrapers diferentes
- [ ] Implementar un sistema de queues con Redis
- [ ] Trabajar con Puppeteer en al menos 2 proyectos
- [ ] Crear APIs REST bien documentadas
- [ ] Integrar PostgreSQL y MongoDB en proyectos
- [ ] Implementar autenticaci√≥n y autorizaci√≥n segura
- [ ] Crear Dockerfiles y usar Docker Compose
- [ ] Estudiar arquitecturas event-driven
- [ ] Practicar debugging y troubleshooting
- [ ] Leer sobre compliance y privacidad de datos
- [ ] Completar el proyecto final integrador

---

## üéì Notas Importantes

1. **√âtica en Web Scraping**: Siempre respeta robots.txt, t√©rminos de servicio, y leyes locales
2. **Performance**: Optimiza desde el inicio, el scraping a escala requiere eficiencia
3. **Error Handling**: En scraping, los errores son comunes - man√©jalos apropiadamente
4. **Testing**: Testea especialmente casos edge (sitios ca√≠dos, HTML malformado, etc.)
5. **Documentaci√≥n**: Documenta tus decisiones t√©cnicas y arquitectura

---

## üöÄ Pr√≥ximos Pasos

1. Comienza con los retos de Nivel 1
2. Avanza a Nivel 2 cuando te sientas c√≥modo
3. Tackle Nivel 3 cuando domines los anteriores
4. Completa el Proyecto Final Integrador
5. Prepara tu portfolio con estos proyectos
6. Estudia el producto de Optery para entender su misi√≥n
7. Prepara preguntas sobre su arquitectura y desaf√≠os t√©cnicos

¬°Buena suerte con tu preparaci√≥n! üí™

